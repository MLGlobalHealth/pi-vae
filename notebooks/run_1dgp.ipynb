{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import cmdstanpy\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ea7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmdstanpy.install_cmdstan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"pi-vae\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.00003,\n",
    "    \"dataset\": \"1d-gp\",\n",
    "    \"epochs\": 20000,\n",
    "    },\n",
    "    name=\"1d-gp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP1D(Dataset):\n",
    "    def __init__(self, dataPoints=100, samples=10000, ingrid=True, x_lim = 1,\n",
    "                        seed=np.random.randint(20), kernel='rbf',ls = 0.1, nu=2.5):\n",
    "        self.dataPoints = dataPoints\n",
    "        self.samples = samples\n",
    "        self.ingrid = ingrid\n",
    "        self.x_lim = x_lim\n",
    "        self.seed = seed\n",
    "        self.Max_Points = 2 * dataPoints\n",
    "        self.ls = ls\n",
    "        self.nu = nu\n",
    "        self.kernel = kernel\n",
    "        np.random.seed(self.seed)\n",
    "        self.evalPoints, self.data = self.__simulatedata__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "    def __getitem__(self, idx=0):\n",
    "        return(self.evalPoints[:,idx], self.data[:,idx])\n",
    "\n",
    "\n",
    "    def __simulatedata__(self):\n",
    "        if self.kernel=='rbf':\n",
    "            gp = GaussianProcessRegressor(kernel=RBF(length_scale=self.ls))\n",
    "        elif self.kernel=='matern':\n",
    "            gp = GaussianProcessRegressor(kernel=Matern(length_scale=self.ls, nu=self.nu))\n",
    "        else:\n",
    "            return None\n",
    "        if (self.ingrid):\n",
    "            X_ = np.linspace(-self.x_lim, self.x_lim, self.dataPoints)\n",
    "            y_samples = gp.sample_y(X_[:, np.newaxis], self.samples)\n",
    "            # print(X_.shape, y_samples.shape)\n",
    "            return (X_.repeat(self.samples).reshape(X_.shape[0],self.samples) ,\n",
    "                        y_samples)\n",
    "        else:\n",
    "            X_ = np.linspace(-self.x_lim, self.x_lim, self.Max_Points)\n",
    "            X_ = np.random.choice(X_, (self.dataPoints,self.samples))\n",
    "            X_.sort(axis=0)\n",
    "            y_samples = np.zeros((self.dataPoints,self.samples))\n",
    "            for idx in range(self.samples):\n",
    "                x_ = X_[:,idx]\n",
    "                y_samples[:,idx] = gp.sample_y(x_[:, np.newaxis]).reshape(self.dataPoints,)\n",
    "            # print(X_.shape, y_samples.shape)\n",
    "            return (X_, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_1D_gp():\n",
    "    dataset =GP1D(dataPoints=100, samples=10000, ls=0.1, x_lim=3)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for no, dt in enumerate(dataloader):\n",
    "        ax.plot(dt[0].reshape(-1,1), dt[1].reshape(-1,1), marker='o', markersize=3)\n",
    "        if no > 9: break\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y=f(x)$')\n",
    "    ax.set_title('10 different function realizations at fixed 100 points\\n'\n",
    "    'sampled from a Gaussian process with RBF')\n",
    "#     fig_image = wandb.Image(fig)\n",
    "    wandb.log({\"data visualization\": fig})\n",
    "    fig\n",
    "\n",
    "visualize_1D_gp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHI(nn.Module):\n",
    "    '''\n",
    "    Implementation of feature transformation layer with RBF layer.\n",
    "    We assume here that alpha is constant for all basis.\n",
    "    Shape:\n",
    "        - Input: (N, n_evals, in_features) N is batches\n",
    "        - Output: (N, n_evals, out_dims), out_dims is a parameter\n",
    "    Parameters:\n",
    "        - in_features: number of input dimension for each eval point\n",
    "        - alpha - trainable parameter controls width. Default is 1.0\n",
    "        - n_centers - number of points to be used as centers in rbf/matern\n",
    "        layers. centers are trainable, default is 100\n",
    "        - hidden_dim1: hidden dimension for 1st layer. Default is 20\n",
    "        - hidden_dim2: hidden dimension for 2nd layer. Default is 20\n",
    "        - out_dims: output features to construct. Default is 100\n",
    "    Examples:\n",
    "        >>> a1 = PHI(256)\n",
    "        >>> x = torch.randn(1,256)\n",
    "        >>> x = a1(x)\n",
    "    '''\n",
    "    def __init__(self, in_features, alpha = 1.0, n_centers = 10, \n",
    "                    hidden_dim1 = 20, hidden_dim2 = 20, out_dims = 100):\n",
    "        '''\n",
    "        Initialization.\n",
    "        INPUT:\n",
    "            - in_features: number of input dimension for each eval point\n",
    "            - alpha: trainable parameter\n",
    "            alpha is initialized with 1.0 value by default\n",
    "            - n_centers: number of points to be used as centers in rbf/matern\n",
    "            layers. centers are trainable, default is 100\n",
    "            - hidden_dim1: hidden dimension for 1st layer. Default is 20\n",
    "            - hidden_dim2: hidden dimension for 2nd layer. Default is 20\n",
    "            - out_dims: hidden dimension for 2nd layer. Default is 100\n",
    "        '''\n",
    "        super(PHI,self).__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        # initialize alpha\n",
    "       # self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha\n",
    "        #self.alpha.requiresGrad = True # set requiresGrad to true!\n",
    "        # centers\n",
    "        self.centers = Parameter(torch.randn(n_centers, in_features)) # create a tensor out of centers\n",
    "        self.centers.requiresGrad = True # set requiresGrad to true!\n",
    "        # linear layers\n",
    "        self.linear1 = nn.Linear(n_centers, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.out = nn.Linear(hidden_dim2, out_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        Applies the function to the input elementwise.\n",
    "        '''\n",
    "        rbf = torch.exp(-1 * torch.cdist(x, self.centers).pow(2))\n",
    "        hidden1 = torch.tanh(self.linear1(rbf))\n",
    "        hidden2 = torch.tanh(self.linear2(hidden1))\n",
    "        out = self.out(hidden2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.mu = nn.Linear(hidden_dim2, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim2, z_dim)\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = torch.tanh(self.linear1(x))\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        hidden2 = torch.tanh(self.linear2(hidden1))\n",
    "        # hidden2 is of shape [batch_size, hidden_dim2]\n",
    "        z_mu = self.mu(hidden2)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden2)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, hidden_dim2, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.out = nn.Linear(hidden_dim2, input_dim)\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = torch.tanh(self.linear1(x))\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        hidden2 = torch.tanh(self.linear2(hidden1))\n",
    "        # hidden2 is of shape [batch_size, hidden_dim2]\n",
    "        pred = self.out(hidden2)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1, hidden_dim2, input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "def calculate_loss_VAE(x, reconstructed_x, mean, log_sd):\n",
    "    # reconstruction loss\n",
    "    RCL = F.mse_loss(reconstructed_x, x, reduction='sum')\n",
    "    # kl divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f55f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIVAE(nn.Module):\n",
    "    '''\n",
    "    Implementation of PIVAE with feature transformation layer (RBF layer).\n",
    "    Shape:\n",
    "        - Input: (N, n_evals, in_features) N is batches\n",
    "        - Output: (N, n_evals, 1), currently we have 1D output only\n",
    "    Parameters:\n",
    "        - in_features: number of input dimension for each eval point\n",
    "        - alpha - trainable parameter controls width. Default is 1.0\n",
    "        - n_centers - number of points to be used as centers in rbf/matern\n",
    "        layers. centers are trainable, default is 100\n",
    "        - dim1: hidden dimension for 1st transformation layer. Default is 20\n",
    "        - dim2: hidden dimension for 2nd layer. Default is 20\n",
    "        - out_dims: output features to construct (size of beta and VAE). \n",
    "        Default is 100\n",
    "        - hidden_dim1 - hidden dimensions for 1st layer VAE. Default is 128\n",
    "        - hidden_dim2 - hidden dimensions for 1st layer VAE. Default is 64\n",
    "        - z_dim - latent dimension for VAE. Default is 20\n",
    "        - batch_size - batch_size for training. For now set same as n_samples\n",
    "    Examples:\n",
    "        >>> a1 = PHI(256)\n",
    "        >>> x = torch.randn(1,256)\n",
    "        >>> x = a1(x)\n",
    "    '''\n",
    "    def __init__(self, in_features, alpha = 1.0, n_centers = 10, dim1 = 20, \n",
    "                    dim2 = 20, out_dims = 100, hidden_dim1 = 128, \n",
    "                    hidden_dim2 = 64, z_dim = 20, batch_size = 10000):\n",
    "        super(PIVAE, self).__init__()\n",
    "        self.out_dims = out_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.phi = PHI(in_features, alpha=alpha, n_centers=n_centers, \n",
    "                        hidden_dim1=dim1, hidden_dim2=dim2, out_dims=out_dims)\n",
    "#         self.betas = nn.ModuleList()\n",
    "#         for _ in range(self.batch_size):\n",
    "#             self.betas.append(nn.Linear(out_dims, 1))\n",
    "        self.betas = Parameter(torch.randn(batch_size, out_dims)) # create a beta matrix\n",
    "        self.betas.requiresGrad = True\n",
    "        \n",
    "        self.vae = VAE(input_dim=out_dims, hidden_dim1=hidden_dim1, \n",
    "                        hidden_dim2=hidden_dim2, latent_dim=z_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        Applies the function to the input elementwise.\n",
    "        '''        \n",
    "        phi_x = self.phi(x)\n",
    "        \n",
    "        y1 = torch.einsum(\"bo,bno->bn\",[self.betas,phi_x])\n",
    "        \n",
    "        beta_vae, z_mu, z_sd = self.vae(self.betas)\n",
    "        \n",
    "        y2 = torch.einsum(\"bo,bno->bn\",[beta_vae,phi_x])\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "\n",
    "        return y1, y2, z_mu, z_sd\n",
    "    \n",
    "def calculate_loss(target, reconstructed1, reconstructed2, mean, log_var):\n",
    "    # reconstruction loss\n",
    "    RCL = F.mse_loss(reconstructed1, target, reduction='sum') + \\\n",
    "                F.mse_loss(reconstructed2, target, reduction='sum')\n",
    "    # kl divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return RCL + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1579c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling points to evaluate functions values\n",
    "\n",
    "x_inf = np.linspace(-1,1,100)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=RBF(length_scale=0.1))\n",
    "\n",
    "y_inf = gp.sample_y(x_inf[:, np.newaxis]).reshape(-1)\n",
    "\n",
    "idx = (x_inf>=-0.9) * (x_inf<=0.9)\n",
    "ll_idx = np.where(idx)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_inf, y_inf, color='blue', alpha=0.5)\n",
    "ax.scatter(x_inf[ll_idx], y_inf[ll_idx], marker='+', color='red', alpha=0.5, s=100)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y=f(x)$')\n",
    "image = wandb.Image(fig)\n",
    "wandb.log({\"posterior\": image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = cmdstanpy.CmdStanModel(stan_file='pivae.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_piVAE():\n",
    "    # Just showing how to use piVAE to learn priors\n",
    "\n",
    "    ###### intializing data and model parameters\n",
    "    n_samples = 10000\n",
    "    in_features = 1\n",
    "    n_evals = 100\n",
    "    n_centers = math.ceil(n_evals/2)\n",
    "    alpha = 1.0\n",
    "    dim1 = 20\n",
    "    dim2 = 20\n",
    "    hidden_dims1 = 16\n",
    "    hidden_dims2 = 8\n",
    "    z_dim = 5\n",
    "    out_dims = 100\n",
    "    batch_size = n_samples\n",
    "\n",
    "    ###### creating data, model and optimizer\n",
    "    train_ds = GP1D(dataPoints=n_evals, samples=n_samples, ls=0.1)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    val_ds = GP1D(dataPoints=n_evals, samples=n_samples, ls=0.1)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = PIVAE(in_features=in_features, alpha=alpha, n_centers=n_centers,\n",
    "                     dim1=dim1, dim2=dim2, out_dims=out_dims, \n",
    "                     hidden_dim1=hidden_dims1, hidden_dim2=hidden_dims2, \n",
    "                     z_dim=z_dim, batch_size=batch_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    epochs = 20000\n",
    "    print(device)\n",
    "    ###### running for 20000 epochs\n",
    "    t = trange(epochs)\n",
    "    for e in t:\n",
    "        # set training mode\n",
    "        \n",
    "        if e == 0 or e%200 == 0: # running inference as a test every 100 epoch\n",
    "            phi = model.phi\n",
    "            \n",
    "            vae = model.vae\n",
    "            vae_decoder = vae.decoder\n",
    "            \n",
    "            stan_data = {'p': 5, \n",
    "                 'p1': 16,\n",
    "                 'p2': 8,\n",
    "                 'n': 100,\n",
    "                 'W1': vae_decoder.linear1.weight.T.cpu().detach().numpy(),\n",
    "                 'B1': vae_decoder.linear1.bias.T.cpu().detach().numpy(),\n",
    "                 'W2': vae_decoder.linear2.weight.T.cpu().detach().numpy(),\n",
    "                 'B2': vae_decoder.linear2.bias.T.cpu().detach().numpy(),\n",
    "                 'W3': vae_decoder.out.weight.T.cpu().detach().numpy(),\n",
    "                 'B3': vae_decoder.out.bias.T.cpu().detach().numpy(),\n",
    "                 'beta_dim' : out_dims,\n",
    "                 'phi_x' : phi(torch.tensor(x_inf.reshape(-1,1)).float().to(device)).cpu().detach().numpy(),\n",
    "                 'y': y_inf.reshape(100,),\n",
    "                 'll_len' : ll_idx[0].shape[0],\n",
    "                 'll_idxs' : ll_idx[0]}\n",
    "            \n",
    "            fit = sm.sample(data=stan_data, iter_sampling=2000, iter_warmup=500, chains=4)\n",
    "            \n",
    "            out = fit.stan_variables()\n",
    "\n",
    "            df = pd.DataFrame(out['y2'])\n",
    "            \n",
    "            datapoints = x_inf\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(datapoints, y_inf, color='black', label='True')\n",
    "            ax.scatter(datapoints[ll_idx], y_inf[ll_idx], s=46,label = 'Observations')\n",
    "            ax.fill_between(datapoints.reshape(datapoints.shape[0]), df.quantile(0.025).to_numpy(), df.quantile(0.975).to_numpy(),\n",
    "                                facecolor=\"blue\",\n",
    "                                color='blue', \n",
    "                                alpha=0.2, label = '95% Credible Interval') \n",
    "            ax.plot(datapoints, df.mean().to_numpy().reshape(-1,1), color='red', alpha=0.7, label = 'Posterior mean')\n",
    "            plt.xlim(-3, 3)\n",
    "            plt.ylim(-2, 2)\n",
    "            ax.set_xlabel('$x$')\n",
    "            ax.set_ylabel('$y=f(x)$')\n",
    "            ax.set_title('Inference fit')\n",
    "            ax.legend()\n",
    "            image = wandb.Image(fig)\n",
    "            wandb.log({\"inference fit\": image})\n",
    "            \n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            target = x[1].float().to(device)\n",
    "            # target = target.view(target.shape[0], target.shape[1], 1)\n",
    "            x = x[0].float().to(device)\n",
    "            x = x.view(x.shape[0], x.shape[1], 1)\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            y1, y2, z_mu, z_sd = model(x) # fwd pass\n",
    "            loss = calculate_loss(target, y1, y2, z_mu, z_sd) # loss cal\n",
    "            loss.backward() # bck pass\n",
    "            total_loss += loss.item() \n",
    "            optimizer.step() # update the weights\n",
    "        loss_logging = total_loss/(n_evals*n_samples)\n",
    "        wandb.log({\"train_loss\": loss_logging})\n",
    "        t.set_description(f'Loss is {total_loss/(n_evals*n_samples):.3}')\n",
    "        \n",
    "        total_val_loss = 0\n",
    "        for i,x in enumerate(val_dl):\n",
    "            target = x[1].float().to(device)\n",
    "            # target = target.view(target.shape[0], target.shape[1], 1)\n",
    "            x = x[0].float().to(device)\n",
    "            x = x.view(x.shape[0], x.shape[1], 1)\n",
    "            y1, y2, z_mu, z_sd = model(x) # fwd pass\n",
    "            loss = calculate_loss(target, y1, y2, z_mu, z_sd) # loss cal\n",
    "            total_val_loss += loss.item() \n",
    "        loss_logging_val = total_val_loss/(n_evals*n_samples)\n",
    "        wandb.log({\"val_loss\": loss_logging_val})\n",
    "        t.set_description(f'Val Loss is {total_val_loss/(n_evals*n_samples):.3}')\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0281e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_piVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"1d-gp-model-new.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa444f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling points to evaluate functions values\n",
    "x_inf = np.linspace(-1,1,100)\n",
    "gp = GaussianProcessRegressor(kernel=RBF(length_scale=0.1))\n",
    "y_inf = gp.sample_y(x_inf[:, np.newaxis]).reshape(-1)\n",
    "\n",
    "idx = (x_inf>=-0.9) * (x_inf<=0.9)\n",
    "ll_idx = np.where(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = model.phi\n",
    "vae = model.vae\n",
    "vae_decoder = vae.decoder\n",
    "stan_data = {'p': 5, \n",
    "                 'p1': 16,\n",
    "                 'p2': 8,\n",
    "                 'n': 60,\n",
    "                 'W1': vae_decoder.linear1.weight.T.detach().numpy(),\n",
    "                 'B1': vae_decoder.linear1.bias.T.detach().numpy(),\n",
    "                 'W2': vae_decoder.linear2.weight.T.detach().numpy(),\n",
    "                 'B2': vae_decoder.linear2.bias.T.detach().numpy(),\n",
    "                 'W3': vae_decoder.out.weight.T.detach().numpy(),\n",
    "                 'B3': vae_decoder.out.bias.T.detach().numpy(),\n",
    "                 'beta_dim' : 10,\n",
    "                 'phi_x' : phi(torch.tensor(x_inf).float()).detach().numpy(),\n",
    "                 'y': y_inf.reshape(60,),\n",
    "                 'll_len' : ll_idx[0].shape[0],\n",
    "                 'll_idxs' : ll_idx[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81daf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.sample(data=stan_data, iter_sampling=2000, iter_warmup=500, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb41c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fit.stan_variables()\n",
    "\n",
    "df = pd.DataFrame(out['y2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = x_inf\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(datapoints, y_, color='black', label='True')\n",
    "ax.scatter(datapoints[ll_idx], y_inf[ll_idx], s=46,label = 'Observations')\n",
    "ax.fill_between(datapoints.reshape(datapoints.shape[0]), df.quantile(0.025).to_numpy(), df.quantile(0.975).to_numpy(),\n",
    "                    facecolor=\"blue\",\n",
    "                    color='blue', \n",
    "                    alpha=0.2, label = '95% Credible Interval') \n",
    "ax.plot(datapoints, df.mean().to_numpy().reshape(-1,1), color='red', alpha=0.7, label = 'Posterior mean')\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y=f(x)$')\n",
    "ax.set_title('Inference fit')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
